---
layout: post
title: "02. Preprocess Titanic Dataset"
subtitle: "Preprocess Titanic Dataset"
categories: tutorial
tag: DA
comments: true
---
# 02. Preprocess Titanic Dataset

![titanic](https://raw.githubusercontent.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/master/%5B1.%20Classification%20Problem-Titanic%5D/imgs/00.titanic.jpg)

### Step 5. Preprocess Titanic Dataset

앞선 데이터 분석으로 프로젝트 목표에 도움이 될 몇 가지 가정과 결정을 내렸습니다. 지금까지는 데이터셋을 직접 수정하지 않았지만, 이제 앞서 내린 가정과 결정을 실행해봅시다.

#### Correcting by dropping features

feature들을 삭제하는 것은 시작으로써 괜찮은 항목입니다. 데이터의 일부를 삭제해 실행 속도를 높이고 분석을 쉽게 만들 수 있기 때문입니다.

우리가 내린 가정과 결정에 따라서 Cabin과 Ticket feature들을 삭제하겠습니다. 이 실행은 training set과 test set에 동일하게 실행되어야 합니다.

```python
print("Before", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)

train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)
test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)
combine = [train_df, test_df]

print("After", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)
```

#### Creating new feature extracting from existing

Name과 PassengerId feature들을 삭제하기 전에 Name feature를 조작하여 title feature을 추출하고 title과 Survived 간의 상관 관계를 분석하려고 합니다.

다음 코드에서는 정규식을 사용해 title feature를 추출합니다. RegEx 패턴 (\ w + \.)는 Name feature에서 .으로 끝나는 첫 번째 단어를 나타냅니다. expand = False 플래그는 DataFrame을 반환합니다.

#### Observations

Title, Age, Survived를 그래프화 할 때 다음과 같은 관찰 사항을 유의해야 합니다.
- 대부분의 title은 Age 그룹을 정확하게 묶습니다. ex) title 값으 Master인 사람의 평균 연령은 5세입니다.
- 특정 title은 대부분 살아남았거나(Mme, Lady, Sir) 그렇지 못했습니다.(Don, Rev, Jonkheer)

#### Decision

- 모델 훈련 시 title feature를 유지하는 것이 좋겠습니다.

```python
for dataset in combine:
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)

print(pd.crosstab(train_df['Title'], train_df['Sex']))
```

많은 title feature을 더 일반적인 이름으로 바꿀 수 있습니다. 혹은 희귀하지 않은 title들을 "Rare"로 바꿀 수도 있겠죠.

```python
for dataset in combine:
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\
 	'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')

    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
    
print(train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())
```

이 범주형 데이터 title을 숫자로 라벨링할 수 있습니다.

```python
title_mapping = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}
for dataset in combine:
    dataset['Title'] = dataset['Title'].map(title_mapping)
    dataset['Title'] = dataset['Title'].fillna(0)

print(train_df.head())
```

이제 데이터셋에서 Name feature을 안전하게 제거할 수 있습니다. 또한 training 데이터셋에는 PassengerId feature가 필요하지 않습니다.

```python
train_df = train_df.drop(['Name', 'PassengerId'], axis=1)
test_df = test_df.drop(['Name'], axis=1)
combine = [train_df, test_df]
print(train_df.shape, test_df.shape)
```

#### Converting a categorical feature

이제 문자열이 포함된 feature들을 숫자 값으로 변환할 수 있습니다. 이는 대부분의 모델에 필수적인 전처리 과정입니다.

먼저 Sex feature를 여성 = 1, 남성 = 0으로 전환하겠습니다.

```python
for dataset in combine:
    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)

print(train_df.head())
```

#### Completing a numerical continuous feature

이제 결측치와 NULL값이 있는 feature를 추정하고 채워야 합니다. 먼저 Age feature에 대해 작업을 수행합니다.

연속적인 수치를 채우기 위해 세 가지 방법을 사용할 수 있습니다.

1. 간단한 방법으로는 평균과 표준편차 사이를 통한 난수를 생성하는 것이 있습니다.
2. 더 정확한 방법으로는 다른 상관관계를 이용하는 것입니다. 이번에는 Age, Gender, Pclass feature을 이용합니다. Pclass와 Gender의 가능한 조합에서 Age값을 추정합니다. ex) Pclass=1 + Gender=0의 Age 중앙값을 구하고, Pclass=1 + Gender=0의 Age 결측치에 대입합니다.
3. 1, 2번을 결합할 수도 있습니다. 중앙값을 사용하는 대신 Pclass와 Gender의 가능한 조합에서 평균과 표준편차를 통한 난수를 사용합니다.

1번과 3번 방법은 random에 의존하기 때문에, 데이터에 노이즈가 생길 수도 있습니다. 그러니 방법 2를 사용합니다.

```python
grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend()
plt.show()
```

![Pclass-Sex-median-Age](https://raw.githubusercontent.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/master/%5B1.%20Classification%20Problem-Titanic%5D/imgs/02.Pclass-Sex-median-Age.jpg)

Pclass와 Sex 조합을 저장할 빈 배열을 준비합니다.

```python
guess_ages = np.zeros((2,3))
print(guess_ages)
```

이제 Sex와 Pclass에서 여섯 가지 조합을 계산합니다.

```python
for dataset in combine:
    for i in range(0, 2):
        for j in range(0, 3):
            guess_df = dataset[(dataset['Sex'] == i) & \
                                  (dataset['Pclass'] == j+1)]['Age'].dropna()

            # age_mean = guess_df.mean()
            # age_std = guess_df.std()
            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)

            age_guess = guess_df.median()

            # Convert random age float to nearest .5 age
            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5
            
    for i in range(0, 2):
        for j in range(0, 3):
            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\
                    'Age'] = guess_ages[i,j]

    dataset['Age'] = dataset['Age'].astype(int)

print(train_df.head())
```

이제 AgeBands feature를 만들고 Survived와의 상관관계를 계산합니다.

```python
train_df['AgeBand'] = pd.cut(train_df['Age'], 5)
print(train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True))
```

이 AgeBands를 기반으로 Age를 라벨링합니다.

```python
for dataset in combine:    
    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0
    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1
    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2
    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3
    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4
print(train_df.head())
```

이제 필요없는 AgeBands feature를 제거합니다.

```python
train_df = train_df.drop(['AgeBand'], axis=1)
combine = [train_df, test_df]
print(train_df.head())
```

#### Create new feature combining existing features

Parch와 SibSp를 결합해 FamilySize feature를 만들고 데이터셋에서 Parch와 SibSp를 삭제합니다.

```python
for dataset in combine:
    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1

print(train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False))
```

가족이 없는 경우는 특별하다고 판단할 수 있습니다. IsAlone feature를 만듭니다.

```python
for dataset in combine:
    dataset['IsAlone'] = 0
    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1

print(train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())
```

IsAlone feature의 성능이 생각보다 더 뛰어납니다. 중복된 데이터는 모델의 정확도를 낮춥니다. Parch, SibSp, FamilySize feature를 삭제합니다.

```python
train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)
test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)
combine = [train_df, test_df]

print(train_df.head())
```

Pclass와 Age를 결합해 feature를 만드는 것도 고려할 수 있습니다.

```python
for dataset in combine:
    dataset['Age*Class'] = dataset.Age * dataset.Pclass

print(train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10))
```

#### Completing a categorical feature

Embarked feature는 S, Q, C 세 가지 값을 가집니다. training dataset에는 두 개의 결측값이 있는데, 이는 가장 일반적인 값으로 채웁니다.

```python
freq_port = train_df.Embarked.dropna().mode()[0]
print(freq_port)
```

```python
for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)
    
print(train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False))
```

#### Converting categorical feature to numeric

이제 Embarked feature를 라벨링합니다.

```python
for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)

print(train_df.head())
```

#### Quick completing and converting a numeric feature

이제 Fare feature의 결측값을 채웁니다. 다음의 코드는 결측치를 보간하는 것이 아닌 다른 하나의 값으로 채우는 행위일 뿐입니다. 이는 그저 모델의 데이터 요구사항을 채우기 위함입니다.

또, 이 Fare feature를 소수점 이하 두 자리로 반올림할 수도 있습니다.

```python
test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)
print(test_df.head())
```

전의 AgeBand와 비슷하게 FareBand를 만듭니다. 

```python
train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)
print(train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True))
```

Fare feature를 FareBand로 라벨링합니다.

```python
for dataset in combine:
    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0
    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1
    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2
    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3
    dataset['Fare'] = dataset['Fare'].astype(int)

train_df = train_df.drop(['FareBand'], axis=1)
combine = [train_df, test_df]
```

데이터 전처리가 끝났습니다! 데이터 형식을 한 번 확인해봅시다.

```python
print(train_df.head(10))
print(test_df.head(10))
```
