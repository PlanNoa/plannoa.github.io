---
layout: post
title: "03. Predict Titanic Dataset"
subtitle: "Predict Titanic Dataset"
categories: tutorial
tag: DA
comments: true
---
# 03. Predict Titanic Dataset

![titanic](https://raw.githubusercontent.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/master/%5B1.%20Classification%20Problem-Titanic%5D/imgs/00.titanic.jpg)

### Step 6. Predict Titanic Dataset

지금까지 딥 러닝 모델에 필요한 데이터 전처리 과정을 거쳤습니다. 지금부터는 딥 러닝 모델을 설계하고 test dataset을 예측해봅시다.

가장 먼저 전처리한 데이터를 training set과 validation set으로 나눕니다. validation set은 모델 학습이 올바른 방향으로 진행되고 있는지를 파악하는데 사용됩니다.

```python
X_train = train_df.iloc[:,1:].values
y_train = train_df.iloc[:, :1].values
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
```

다음으로 딥 러닝 라이브러리 pytorch를 사용해 모델을 설계합니다. `torch.manual_seed(0)`은 코드 실행이 항상 같은 결과를 만들게 합니다.

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.autograd import Variable

torch.manual_seed(0)
```

pytorch의 모델은 기본적으로 `torch.nn.Module` 클래스를 상속해 만듭니다. 이렇게 만든 자식 클래스는 부모 클래스의 `forward` 함수를 구현해야 사용할 수 있습니다. `forward` 함수는 모델에 input을 넣고 output을 만드는 과정을 넣어야 합니다. 함수 내부의 net들은 `__init__`에서 선언해야 합니다.

모델은 간단하게 설계합니다. 4개의 fully conneted layer를 만들고, 사이사이에 Dropout을 끼워 Overfitting을 방지합니다.

```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(8, 512)
        self.fc2 = nn.Linear(512, 1024)
        self.fc3 = nn.Linear(1024, 512)
        self.fc4 = nn.Linear(512, 2)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x - self.dropout(x)
        x = self.fc4(x)
        return x
```

자, 모델을 만들었으니 만든 모델, 손실 함수와 optimizer를 선언합니다. 편하게 pytorch 패키지 내부에 구현된 것들을 사용합시다.

Cross Entropy에 관한 설명은 [여기](https://wordbe.tistory.com/entry/ML-Cross-entropyCategorical-Binary%EC%9D%98-%EC%9D%B4%ED%95%B4)를 참고하시기 바랍니다. optimizer는 가장 기본적인 Stochastic gradient descent를 사용합니다. optimizer 선언에 사용된 파라미터 lr는 learning rate를 의미합니다. 학습 속도에 따라 조정할 수 있습니다.

```python
model = Net()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

DataLoader를 사용하지 않으므로 batch size를 임의로 정하고, 총 배치 수도 선언해야 합니다. 

Batch를 사용하는 이유는 [여기](https://dambaekday.tistory.com/1)를 참고하시기 바랍니다.

```python
batch_size = 64
n_epochs = 1000
batch_no = len(X_train) // batch_size
```

준비는 다 끝났습니다. 모델 학습을 시작합시다.

Batch마다 학습하고, 한 epoch마다 validation data의 loss가 이전보다 낮아지면 학습된 모델을 저장합니다. 100 epoch마다 validation data의 accuracy를 출력합니다.

pytorch에 구현된 CrossEntropy loss는 output으로 FloatTensor를, target으로 LongTensor를 받습니다.

```python
val_loss_min = np.inf
for epoch in range(n_epochs):
    for i in range(batch_no):
        start = i * batch_size
        end = start + batch_size
        x_var = Variable(torch.FloatTensor(X_train[start:end]))
        y_var = Variable(torch.LongTensor(y_train[start:end]).squeeze())

        optimizer.zero_grad()
        output = model(x_var)
        loss = criterion(output, y_var)
        loss.backward()
        optimizer.step()

    X_val = Variable(torch.FloatTensor(X_val))
    y_val = Variable(torch.LongTensor(y_val))
    val_output = model(X_val)
    val_loss = criterion(val_output, y_val.squeeze())

    if val_loss <= val_loss_min:
        print("Validation loss decreased ({:6f} ===> {:6f}). Saving the model...".format(val_loss_min, val_loss))
        torch.save(model.state_dict(), "output/model.pt")
        val_loss_min = val_loss

    if epoch % 100 == 0:
        values, labels = torch.max(val_output, 1)
        num_right = np.sum(labels.data.numpy() == y_val.squeeze().numpy())

        print('')
        print("Epoch: {} \tValidation Loss: {} \tValidation Accuracy: {}".format(epoch+1, val_loss, num_right / len(y_val)))

print('Training Ended! ')
```

모델 학습이 성공적으로 완료되었으면 제출할 test data를 처리합니다.

```python
X_test = test_df.iloc[:,1:].values
X_test = Variable(torch.FloatTensor(X_test))
with torch.no_grad():
    test_result = model(X_test)
values, labels = torch.max(test_result, 1)
survived = labels.squeeze().numpy()
```

output을 submission.csv 파일명으로 저장합니다.

```python
submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': survived})
submission.to_csv('output/submission.csv', index=False)
```

저장한 파일을 제출해 score를 확인합니다. [이곳](https://www.kaggle.com/c/titanic/submissions)에서 제출합니다.

![Score](https://raw.githubusercontent.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/master/%5B1.%20Classification%20Problem-Titanic%5D/imgs/03.EMNIST.JPG)

대략 78%의 정확도를 가지는 모델입니다.

지금까지 보여드린 코드에는 데이터를 미숙하게 처리하는 등 개선할 부분이 많이 있습니다. 이 코드들을 기반으로 자신의 모델을 만들어 정확도를 개선해보세요.
