---
layout: post
title: "01. Handling Houses Prices Dataset"
subtitle: "Handling Houses Prices Dataset"
categories: tutorial
tag: DA
comments: true
---
# 01. Handling Houses Prices Dataset

![Houses Prices](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/00.houses%20prices.jpg)

### Step 1. Define the Problem

Project Summary: 부동산에 가서 집을 구매하는 상황을 생각해봅시다. 부동산 중개인은 당신에게 어떤 집을 원하냐고 물어볼 것이고, 여러분은 방의 개수, 층의 개수, 근처 편의시설.... 등 많은 말을 꺼낼 것입니다.

하지만 이 프로젝트는 그런 것보다도 훨씬 많은 조건(ex) 지하실 높이, 담벼락 높이 등)이 집값에 영향을 미친다는 사실을 알려줍니다!

이 데이터는 기본적으로 아이오와 주 에임스에 있는 모든 주거용 주택의 모든 측변을 설명합니다. 이번 프로젝트에서, 총 79개의 feature로 각 주택의 최종 가격 예측을 목표로 합니다.

### Step 2. Gather the Data

Download data from [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)

### Step 3. Analyse & Handling Houses Prices Data

패키지들을 import하고 output폴더를 만듭니다.

``` python
  
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import seaborn as sns
from scipy import stats
from scipy.stats import norm, skew #for some statistics
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

import os
if not os.path.exists('../output'):
    os.mkdir('../output')
```

#### Acquire data

이전 타이타닉 때와 똑같이, train/test dataset을 불러옵니다.

```python
train_df = pd.read_csv('../input/train.csv')
test_df = pd.read_csv('../input/test.csv')
```

#### Analyze by describing data

이번 프로젝트에서 활용할 feature들을 봅시다. 각 feature들에 대한 설명은 다운받은 `data_description.txt` 파일에 적혀있습니다.

```python
print(train_df.head(5))
print(test_df.head(5))
```

먼저 sample 번호를 나타내는 Id feature를 제거해줍니다.

```python
#check the numbers of samples and features
print("The train data size before dropping Id feature is : {} ".format(train_df.shape))
print("The test data size before dropping Id feature is : {} ".format(test_df.shape))

#Save the 'Id' column
train_ID = train_df['Id']
test_ID = test_df['Id']

#Now drop the  'Id' colum since it's unnecessary for  the prediction process.
train_df.drop("Id", axis = 1, inplace = True)
test_df.drop("Id", axis = 1, inplace = True)

#check again the data size after dropping the 'Id' variable
print("\nThe train data size after dropping Id feature is : {} ".format(train_df.shape)) 
print("The test data size after dropping Id feature is : {} ".format(test_df.shape))
```

#### 데이터에 outlier가 있는 경우

Outlier란 특이값, 이상값이라고도 불리며 '잘못 평가된 값'이라는 의미로 사용합니다. 예를 들어 아래 그림을 봅시다. 이 그림은 고객들의 수입을 boxplot으로 표현한 것입니다. 수입의 분포는 약 0.8million$가 매년 수입의 평균을 나타냅니다.

![Outlier-1](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Outlier-1.JPG)

Outlier의 종류는 Univariate와 Multivariate 두 가지로 나눌 수 있습니다. Univariate는 위에서 언급한 상황처럼 하나의 변수 분포에서 나타나는 outlier를 의미합니다. 반대로 Multivariate는 n개의 차원에서 나타나는 outlier입니다. 이를 발견하기 위해서는 n개 차원에서의 분포를 확인해야 합니다.

![Outlier-2](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Outlier-2.JPG)

#### Outlier의 원인

Outlier가 발견하면, 그 outlier의 원인을 알아야 해결할 수 있습니다. outlier의 원인에는 다음과 같은 것들이 있습니다.

- Artificial/Non-Natural
  - Data Entry Error: 사람이 데이터를 수집하는 과정에서 발생하는 에러입니다. 숫자 데이터의 경우에는 한 글자만 더 들어가도 10배로 입력이 됩니다. 이런 값은 전체 샘플 데이터의 분포를 볼 때 쉽게 확인할 수 있습니다.
  - Measurement Error: 데이터를 측정하는 과정에서 발생하는 에러입니다. 예를 들어 체온을 측정하는데 9개는 정상 동작, 1개는 비정상 동작할때 그 비정상 동작하는 체온계를 사용할 경우 이런 에러가 발생합니다.
  - Experimental Error: 실험을 할 때 발생하는 에러입니다. 환경 변수를 통제하지 못했을 때 발생할 수 있습니다.
  - Intentional Outlier: 자기 자신에게 조사할 때 발생할 수 있는 에러입니다. 남자들에게 키를 설문조사할 때, 의식적으로 몇 cm 크게 답할 수 있습니다. 이처럼 민감한 질문에 대해 거짓으로 답할 경우 생기는 error입니다.
  - Sampling Error: 데이터를 샘플링하는 경우에 나타나는 에러로, 축구선수들의 키를 측정할 때 만약 농구선수를 함께 포함하는 샘플링을 하는 경우 농구선수들은 outlier로 생각할 수 있습니다.
- Natural
  - Natural Outlier: 위처럼 인공적이 아니게 발생하는 outlier를 말합니다.
  
#### Outlier가 데이터에 미치는 영향

Outliers는 데이터 분석이나 통계, 모델링의 결과에 심각한 변화를 줄 수 있습니다. 그 영향들을 아래 정리하자면,

- error varience를 증가시키고 statistical tests의 power를 감소시킵니다.
- outliers가 non-randomly 분포를 보이면 normality를 감소시킵니다.
- regression, ANOVA 등의 모델에 영향을 줄 수 있습니다.

이를 깊게 이해하기 위해 예시를 보자면, 

![Outlier-3](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Outlier-3.JPG)

outlier가 있는 경우에 상당히 다른 mean, standard deviation 값을 나타내는 것을 볼 수 있습니다. 이는 추정값이 완전히 틀려짐을 의미합니다.

#### Outlier를 발견하는 방법

대부분의 Outlier는 시각화를 통해서 발견됩니다. 위의 예시처럼 box plot, scatter plot을 사용하거나, histogram을 사용할 수도 있습니다.

#### Outlier를 처리하는 방법

Outlier를 처리하는 방법은 이전에 결측치를 채우는 것처럼 관측된 값을 제거하거나, 구간화 처리를 하면 됩니다. 아래에 더 자세한 방법들을 설명하겠습니다.

- Deleting Observation: outlier 값을 제거하는 방법입니다.
- Transforming and Binning values: Transforming value는 extreme values로 인한 outlier에 자연로그를 취해  값을 변형하는 방법입니다. Binning value는 구간평균 혹은 평활화 방법을 통한 bucket을 적용한 것을 말합니다. Decision Tree(의사결정 나무)의 경우에는 Binning value를 사용하는 것과 같습니다.
- Imputing: missing value를 imputation하는 것과 동일합니다.(titanic dataset에서 사용한 방법) imputing value 전에 outlier가 artificial인지 natural인지를 분석해야 합니다. artificial이라면 imputing value를 사용할 수 있습니다.
- Treat Separately: outlier가 많을 경우 사용합니다. 그룹을 두 개로 나눠 접근하는 방식으로, 각 그룹에 대해 model을 생성하고 output을 결합하면 됩니다.

본론으로 돌아갑시다! 지금까지 outlier에 대해 이야기한 이유는 이번 데이터셋에 outlier가 포함되어 있기 때문입니다. 시각화를 통해 찾아봅시다.

```python
fig, ax = plt.subplots()
ax.scatter(x = train_df['GrLivArea'], y = train_df['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
```

![Outlier-4](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Outlier-4.jpg)

오른쪽 하단에 엄청나게 큰 값의 GrLivArea를 가지고, 가격은 저렴한 주택을 볼 수 있습니다. 대놓고 outlier이기 때문에 삭제하는 방법을 사용하겠습니다.

```python
#Deleting outliers
train_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)

#Check the graphic again
fig, ax = plt.subplots()
ax.scatter(train_df['GrLivArea'], train_df['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
```

![Outlier-5](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Outlier-5.jpg)

안전하게 삭제된 모습을 볼 수 있습니다. 다른 방법 대신에 제거하는 방법을 선택한 이유는 언제나 안전한 방법이기 때문입니다.

#### Target feature

SalePrice feature은 우리가 예측해야 하는 목표입니다. 이 feature를 먼저 분석해봅시다.

```python
sns.distplot(train_df['SalePrice'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(train_df['SalePrice'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(train_df['SalePrice'], plot=plt)
plt.show()
```

![SalePrice-1](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.SalePrice-1.jpg)
![SalePrice-2](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.SalePrice-2.jpg)

목표 feature가 치우쳐있는 모습을 보입니다. 보통 정규 분포 데이터가 예측하기에 더 쉽기 때문에, SalePrice feature를 변환하여 정규분포로 만들겠습니다.

#### Log-transformation of the target variable

```python
#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column
train_df["SalePrice"] = np.log1p(train_df["SalePrice"])

#Check the new distribution
sns.distplot(train_df['SalePrice'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(train_df['SalePrice'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(train_df['SalePrice'], plot=plt)
plt.show()
```

![SalePrice-3](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.SalePrice-3.jpg)
![SalePrice-4](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.SalePrice-4.jpg)

깔끔하고 이쁜 모양입니다. 편향된 데이터가 수정되었고, 보다 더 정규 분포를 따르는 것으로 보입니다.

#### Features engineering

먼저 train과 test dataset을 concat 합니다.

```python
ntrain = train.shape[0]
ntest = test.shape[0]
y_train = train.SalePrice.values
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['SalePrice'], axis=1, inplace=True)
print("all_data size is : {}".format(all_data.shape))
```

#### Missing Data

NULL data의 feature별 비율을 살펴봅시다.

```python
combine_na = (combine.isnull().sum() / len(combine)) * 100
combine_na = combine_na.drop(combine_na[combine_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' :combine_na})
print(missing_data.head(20))
```

이렇게 보니 체감이 잘 되지 않는군요. 시각화를 해 보죠.

```python
f, ax = plt.subplots(figsize=(15, 12))
plt.xticks(rotation='90')
sns.barplot(x=combine_na.index, y=combine_na)
plt.xlabel('Features', fontsize=15)
plt.ylabel('Percent of missing values', fontsize=15)
plt.title('Percent missing data by feature', fontsize=15)
plt.savefig("https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Percent missing data.jpg")
plt.show()
```

![Percent missing data](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Percent%20missing%20data.jpg)

이제 NULL data를 채웁니다. titanic때와는 달리 feature들에 세세한 설명이 들어있습니다. 설명을 기반으로 데이터를 채웁시다.

- PoolQC: 이 feature에서 NULL은 주택에 수영장이 없음을 의미합니다. 99% 이상의 데이터가 NULL이고, 대부분의 주택에는 수영장이 없음을 감안하면 말이 되는군요. NULL도 하나의 데이터가 되는 경우이므로 None이라는 텍스트를 채워줍니다.

```python
all_data["PoolQC"] = all_data["PoolQC"].fillna("None")
```

- MiscFeature: 이 feature은 PoolQC와 비슷하게 NULL값이 기타 기능 없음을 뜻합니다. PoolQC와 똑같이 처리해줍니다.

```python
combine["MiscFeature"] = combine["MiscFeature"].fillna("None")
```

- 뒤의 세 feature Alley, Fence, FireplaceQU도 똑같은 의미이므로 None으로 채워줍니다.

```python
combine["Alley"] = combine["Alley"].fillna("None")
combine["Fence"] = combine["Fence"].fillna("None")
combine["FireplaceQu"] = combine["FireplaceQu"].fillna("None")
```

- LotFrontage: 이 feature는 주택과 연결된 거리의 수를 의미합니다. 주변 Neighborhood Lotfrontage들의 중앙값을 사용합니다.

```python
#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood
combine["LotFrontage"] = combine.groupby("Neighborhood")["LotFrontage"].transform(
    lambda x: x.fillna(x.median()))
```

- GarageType, GarageFinish, GarageQual and GarageCond: None으로 채웁니다.

```python
for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):
    combine[col] = combine[col].fillna('None')
```

- GarageYrBlt, GarageArea and GarageCars: 차고가 없다는 의미는 차고에 자동차가 없다는 의미이므로 0으로 채웁니다.

```python
for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    combine[col] = combine[col].fillna(0)
```

- BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath: 지하실이 없으면 결측값도 0일 가능성이 높습니다.

```python
for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
    combine[col] = combine[col].fillna(0)
```

- BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2: 이런 범주형 데이터들이 없다는 것은 지하실이 없다는 것을 의미합니다.

```python
for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    combine[col] = combine[col].fillna('None')
```

- MasVnrArea and MasVnrType: 이 feature에서 NULL이라는 의미는 석조 장식이 없다는 사실을 의미합니다. None과 0으로 채워줍니다.

```python
combine["MasVnrType"] = combine["MasVnrType"].fillna("None")
combine["MasVnrArea"] = combine["MasVnrArea"].fillna(0)
```

- MSZoning(구역 분류): 가장 일반적인 값 'RL'로 채워줍니다.

```python
combine['MSZoning'] = combine['MSZoning'].fillna(combine['MSZoning'].mode()[0])
```

- Utilities: 이 feature은 1개의 'NoSeWa', 2개의 NULL을 제외한 나머지는 모두 'AllPub'입니다. 심지어 'NoSeWa'는 training dataset에만 포함되어 있습니다. 2개의 NULL은 무시해도 무방하다고 생각되니 그냥 삭제합니다.

```python
combine = combine.drop(['Utilities'], axis=1)
```

- Functional: 여기서 NULL은 'typical'이라는 의미를 가집니다.

```python
combine["Functional"] = combine["Functional"].fillna("Typ")
```

- Electrical, KitchenQual, Exterior1st ,Exterior2nd, SaleType : 하나의 NULL을 가지고 있습니다. 가장 많은 값으로 채워줍니다.

```python
combine['Electrical'] = combine['Electrical'].fillna(combine['Electrical'].mode()[0])
combine['KitchenQual'] = combine['KitchenQual'].fillna(combine['KitchenQual'].mode()[0])
combine['Exterior1st'] = combine['Exterior1st'].fillna(combine['Exterior1st'].mode()[0])
combine['Exterior2nd'] = combine['Exterior2nd'].fillna(combine['Exterior2nd'].mode()[0])
combine['SaleType'] = combine['SaleType'].fillna(combine['SaleType'].mode()[0])
```

- MSSubClass: NULL은 건물 등급 없음을 의미합니다. None으로 대체합니다.

```python
combine['MSSubClass'] = combine['MSSubClass'].fillna("None")
```

중복되는 코드가 너무 많군요. 하나로 다 줄여버립시다.

```python
combine["LotFrontage"] = combine.groupby("Neighborhood")["LotFrontage"].transform(
    lambda x: x.fillna(x.median()))

for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 
            'GarageFinish', 'GarageQual', 'GarageCond', 'MasVnrType', 'MSSubClass', 
            'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    combine[col] = combine[col].fillna('None')

for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea', 'BsmtFinSF1', 
            'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
    combine[col] = combine[col].fillna(0)

for col in ('Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'MSZoning'):
    combine[col] = combine[col].fiilna(combine[col].mode()[0])
    
combine = combine.drop(['Utilities'], axis=1)
combine["Functional"] = combine["Functional"].fillna("Typ")
```

남은 NULL data가 있나 확인합니다.

```python
combine_na = (combine.isnull().sum() / len(combine)) * 100
combine_na = combine_na.drop(combine_na[combine_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio' :combine_na})
print(missing_data.head())
```

#### Data Correlation

데이터들 사이의 상관관계를 확인합니다.

```python
#Correlation map to see how features are correlated with SalePrice
corrmat = train_df.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)
plt.show()
```

![Correlation map](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.Correlation%20map.jpg)

#### More features engeneering

#### Transforming some numerical variables that are really categorical

데이터 전처리 과정을 위해 사실상 범주형 데이터인 숫자형 데이터를 수정한다.

```python
#MSSubClass=The building class
combine['MSSubClass'] = combine['MSSubClass'].apply(str)

#Changing OverallCond into a categorical variable
combine['OverallCond'] = combine['OverallCond'].astype(str)

#Year and month sold are transformed into categorical features.
combine['YrSold'] = combine['YrSold'].astype(str)
combine['MoSold'] = combine['MoSold'].astype(str)
```

#### Label Encoding some categorical variables that may contain information in their ordering set

```python
from sklearn.preprocessing import LabelEncoder
cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',
        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',
        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',
        'YrSold', 'MoSold')
# process columns, apply LabelEncoder to categorical features
for c in cols:
    lbl = LabelEncoder()
    lbl.fit(list(combine[c].values))
    combine[c] = lbl.transform(list(combine[c].values))

# shape
print('Shape all_data: {}'.format(combine.shape))
```

#### Adding one more important feature

주택 가격을 결정짓는 요소 중 큰 비율을 차지하는 것은 총 면적이다. 그러므로 지하실, 1층, 2층의 면적을 모두 합한 TotalSF feature를 생성해준다.

```python
combine['TotalSF'] = combine['TotalBsmtSF'] + combine['1stFlrSF'] + combine['2ndFlrSF']
```

#### Skewed features

다른 feature들도 Sales Price feature의 기울어진 분포를 가운데로 맞춘 것과 비슷하게 처리합니다. 이런 기울어진 데이터를 처리하는 것은 모델이 y를 더 쉽게 예측하도록 돕습니다.

먼저 feature들의 기울어짐 정도를 계산합니다.

```python
numeric_feats = combine.dtypes[combine.dtypes != "object"].index

# Check the skew of all numerical features
skewed_feats = combine[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness = pd.DataFrame({'Skew' :skewed_feats})
print(skewness.head(10))
```

#### Box Cox Transformation of highly skewed features

크게 기울어진 feature들을 Box-Cox 변환을 통해 정리합니다. Scipy 패키지의 boxcox1p를 사용합니다. boxcox1p는 데이터를 `1+x` 형식으로 변환합니다.

boxcox1p에 관한 자세한 정보는 [여기](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.boxcox1p.html)에서, boxcox에 관한 정보는 [여기](http://onlinestatbook.com/2/transformations/box-cox.html)에서 자세히 볼 수 있습니다.

```python
skewness = skewness[abs(skewness) > 0.75]
print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))

from scipy.special import boxcox1p
skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    combine[feat] = boxcox1p(combine[feat], lam)
```

#### Getting dummy categorical features

pandas 패키지의 get_dummies 함수는 범주형 데이터를 처리할 때 사용합니다. Transforming some numerical variables that are really categorical 단계에서 범주형 데이터를 string(object) 형식으로 바꾸었던 것이 이 get_dummies 함수를 사용하기 위해서였습니다.

get_dummies는 object(숫자형 데이터 제외한 모든 데이터)형 데이터를 one-hot encoding 형식으로 바꿔줍니다. 아래 그림이 one-hot encoding 방식입니다. 모든 데이터를 0과 1, True와 False로 표현합니다.

prefix 파라미터로 column을 지정하면 숫자형 데이터도 범주형으로 바꿀 수 있습니다. Titanic의 Age feature를 넣을 수 있죠.

![One-hot](https://github.com/PlanNoa/Deep-Learning-from-Scratch-for-newbie/raw/master/%5B2.%20Regression%20Problem-Houses%20Prices%5D/imgs/01.One-hot%20encoding.jpg)

```python
combine = pd.get_dummies(combine)
print(combine.shape)
```

이렇게 get_dummies 함수를 사용하면 Titanic 데이터 처리에서 했던 고생을 하지 않아도 됩니다. 개고생만 했군요!

지금까지 Houses Prices Dataset Handling을 끝냈습니다. 마지막의 get_dummies 함수때문에 많이 혼란스러우실 것 같은데, 이에 관해서는 범주형 데이터와 One-Hot encodeing의 이점을 잘 생각해보시길 바랍니다.
